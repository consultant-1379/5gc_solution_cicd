parameters:
  ################################### Network variables #######################
  timezone: CET
  ntp_server_ips: [10.221.17.22]
  dns_server_ips: [10.221.16.10]
  os_endpoint_ips: [10.63.88.186, 10.221.14.88, 214.13.134.139]
  director_external_network: eccd1-ccd-oam
  director_external_subnet: eccd1-ccd-oam_subnet
  security_groups_enabled: False
  # The list of remote CIDRs allowed to SSH to directors when SG are enabled
  #director_ssh_allowed_cidrs: [<fill>]
  # VRRP traffic might need to be specifically enabled when security groups are
  # enabled on deployments running Openstack older than Pike release
  #allow_vrrp_address_by_mac: False
  #receive_packet_steering: false

  ### Ingress/egress via LB ###################################################
  lb_enabled: False
  lb_external_network: <fill>
  lb_external_subnet: <fill>
  # lb_external_security_groups: [<fill>]

  ##################################### L4LB variables ########################

  lb_image: node-image
  lb_flavor: m1.medium

  lb_root_volume_size: 20

  ################################### Master variables ########################
  master_image: eccd-2.22.0-node-image.vbi
  master_flavor: MGMT_8vcpu_16384MBmem_0GBdisk
  masters_count: 3

  master_root_volume_size: 50
  master_config_volume_size: 15

  # managed-on-host, ha-offline, unmanaged
  master_metadata:
    ha-policy: ha-offline

  ################################### Director variables ######################
  director_image: eccd-2.22.0-director-image.vbi
  director_flavor: MGMT_2vcpu_8192MBmem_0GBdisk
  directors_count: 2

  director_root_volume_size: 30
  director_config_volume_size: 24

  director_virtual_router_id: 50
  director_external_vip: 10.63.80.6

  # managed-on-host, ha-offline, unmanaged
  director_metadata:
    ha-policy: ha-offline

  ################################## Licensing variables ######################
  ccd_licensing_domains:
  - productType: ECCD
    swltId: STA-CCD-1
    customerId: 946060
  ccd_licensing_nels_host: 10.221.14.88

  ################################## Ansible variables ######################
  ansible_variables:
    openstack_auth_url: https://n67-vpod1-ctrl.sc.sero.gic.ericsson.se:5000
    infra_node_pool_name: pool1
    kube_manager_kube_api_burst: 100
    kube_manager_kube_api_qps: 100
    kube_manager_attach_detach_reconcile_sync_period: 5m0s
    kube_manager_disable_attach_detach_reconcile_sync: True
    openstack_username: admin
    openstack_user_password: rfwmtGdqAqueuosdrgWOSr1s
    openstack_project_name: admin
    openstack_domain_name: Default
    openstack_region: RegionOne
    ecfe_enabled: true
    ingressctrl_service_annotations: 'metallb.universe.tf/address-pool: system-pool'
    ecfe_config_map_raw:
      bgp-bfd-peers:
      - peer-address: 11.110.2.254
        peer-asn: 3802
        my-asn: 102
        hold-time: 180s
        min-rx: 300ms
        min-tx: 300ms
        multiplier: 3
        my-address-pools:
        - pcc-smf-nsmf
        - pcc-smf-notification
        - ccdm-5g-traffic
        - ccsm-ausf-5g-sig
        - ccrc-nrf-sig
        - ccsm-udm-5g-sig
        - pcc-amf-namf
        - ccrc-nssf-sig
        - sc-bsf-sig
        - sc-bsf-dia-sig
        - ccsm-dia-sig
        - ccsm-eir-5g-sig
        - ccsm-hss-epc-http-sig
        - sc-scp-sig
        - cces-5g-sbi-traffic
        - cces-5g-nbi-trust-traffic
      - peer-address: 10.143.130.254
        peer-asn: 10832
        my-asn: 103
        hold-time: 180s
        min-rx: 300ms
        min-tx: 300ms
        multiplier: 3
        my-address-pools:
        - system-pool
        - pcc-oam
        - pcg-oam
        - ccdm-oam
        - ccdm-5g-prov
        - ccdm-4g-prov
        - ccsm-oam
        - ccrc-oam
        - eda-oam
        - sc-oam
        - cces-oam
        - cces-5g-prov
        node-selectors:
        - match-labels:
            type: standard
      - peer-address: 11.21.93.254
        peer-asn: 8018
        my-asn: 7018
        hold-time: 180s
        min-rx: 300ms
        min-tx: 300ms
        multiplier: 3
        my-address-pools:
        - ccdm-intersite
        - ccdm-4g-traffic
      address-pools:
      - name: pcc-smf-nsmf
        protocol: bgp
        addresses:
        - 5.8.6.129/32
        auto-assign: false
      - name: pcc-smf-notification
        protocol: bgp
        addresses:
        - 5.8.6.130/32
        auto-assign: false
      - name: ccdm-5g-traffic
        protocol: bgp
        addresses:
        - 5.8.6.132/32
        auto-assign: false
      - name: ccsm-ausf-5g-sig
        protocol: bgp
        addresses:
        - 5.8.6.133/32
        auto-assign: false
      - name: ccrc-nrf-sig
        protocol: bgp
        addresses:
        - 5.8.6.134/32
        auto-assign: false
      - name: ccsm-udm-5g-sig
        protocol: bgp
        addresses:
        - 5.8.6.135/32
        auto-assign: false
      - name: pcc-amf-namf
        protocol: bgp
        addresses:
        - 5.8.6.137/32
        auto-assign: false
      - name: ccrc-nssf-sig
        protocol: bgp
        addresses:
        - 5.8.6.138/32
        auto-assign: false
      - name: sc-bsf-sig
        protocol: bgp
        addresses:
        - 5.8.6.139/32
        auto-assign: false
      - name: sc-bsf-dia-sig
        protocol: bgp
        addresses:
        - 5.8.6.140/32
        auto-assign: false
      - name: ccsm-dia-sig
        protocol: bgp
        addresses:
        - 5.8.6.141/32
        auto-assign: false
      - name: ccsm-eir-5g-sig
        protocol: bgp
        addresses:
        - 5.8.6.142/32
        auto-assign: false
      - name: ccsm-hss-epc-http-sig
        protocol: bgp
        addresses:
        - 5.8.6.131/32
        auto-assign: false
      - name: sc-scp-sig
        protocol: bgp
        addresses:
        - 5.8.6.143/32
        auto-assign: false
      - name: cces-5g-sbi-traffic
        protocol: bgp
        addresses:
        - 5.8.6.144/32
        auto-assign: false
      - name: cces-5g-nbi-trust-traffic
        protocol: bgp
        addresses:
        - 5.8.6.145/32
        auto-assign: false
      - name: cces-oam
        protocol: bgp
        addresses:
        - 10.136.11.226/32
        auto-assign: false
      - name: cces-5g-prov
        protocol: bgp
        addresses:
        - 10.136.11.227/32
        auto-assign: false
      - name: system-pool
        protocol: bgp
        addresses:
        - 10.136.11.228/32
        auto-assign: false
      - name: pcc-oam
        protocol: bgp
        addresses:
        - 10.136.11.229/32
        auto-assign: false
      - name: pcg-oam
        protocol: bgp
        addresses:
        - 10.136.11.236/32
        auto-assign: false
      - name: ccdm-oam
        protocol: bgp
        addresses:
        - 10.136.11.230/32
        auto-assign: false
      - name: ccdm-5g-prov
        protocol: bgp
        addresses:
        - 10.136.11.231/32
        auto-assign: false
      - name: ccdm-4g-prov
        protocol: bgp
        addresses:
        - 10.136.11.225/32
        auto-assign: false
      - name: ccsm-oam
        protocol: bgp
        addresses:
        - 10.136.11.232/32
        auto-assign: false
      - name: ccrc-oam
        protocol: bgp
        addresses:
        - 10.136.11.233/32
        auto-assign: false
      - name: eda-oam
        protocol: bgp
        addresses:
        - 10.136.11.235/32
        auto-assign: false
      - name: sc-oam
        protocol: bgp
        addresses:
        - 10.136.11.237/32
        auto-assign: false
      - name: ccdm-intersite
        protocol: bgp
        addresses:
        - 5.8.6.161/32
        auto-assign: false
      - name: ccdm-4g-traffic
        protocol: bgp
        addresses:
        - 5.8.6.162/32
        auto-assign: false
    kube_network_extra_plugin: multus
    ipv4pool_ipip_mode: "Off"
    openstack_node_volume_attach_limit: 20
    kube_api_ingress_host: kubeapi.ingress.n67-eccd1.sero.gic.ericsson.se
    container_registry_hostname: container-registry.ingress.n67-eccd1.sero.gic.ericsson.se
    container_registry_custom_pw: c0ntainerReg!stry
    prometheus_webhook_snmp_enabled: True
    snmp_trap_receiver_host: 214.13.134.139

    # Victoria resources parameters
    pm_vmstorage_retention_time: 30d
    pm_vmstorage_volume_size: 200Gi
    pm_vmagent_memory_limit: 700Mi
    pm_vmselect_cpu_limit: 1
    pm_vmselect_memory_limit: 700Mi
    pm_vmstorage_memory_limit: 7Gi

    # Other monitor component parameters    
    pm_server_nodeExporter_default_collectors_disabled: false

    # Some NFs include a requirement on CoreDNS cache TTL
    kube_dns_success_ttl: 5
    kube_dns_denial_ttl: 5
    nodelocalDNS_config_map_fwdzones:
    - domain: 3gppnetwork.org
      dns_server: 11.0.18.1

    sriov_network_device_plugin_enabled: true
    sriov_network_device_plugin_configs:
      pool2:
        node_selectors: >
          { "node-pool": "pool2" }
        sriov_network_device_plugin_configmap_name: "sriov-dp-configmap2"
        sriov_network_device_plugin_configmap_data: >
          {
              "resourceList": [{
                      "resourceName": "intel_sriov_dpdk_left",
                      "selectors": {
                          "pciAddresses": ["0000:00:06.0", "0000:00:08.0", "0000:00:0a.0", "0000:00:0c.0"],
                          "drivers": ["vfio-pci", "igb_uio"]
                      }
                  },
                  {
                      "resourceName": "intel_sriov_dpdk_right",
                      "selectors": {
                          "pciAddresses": ["0000:00:07.0", "0000:00:09.0", "0000:00:0b.0", "0000:00:0d.0"],
                          "drivers": ["vfio-pci", "igb_uio"]
                      }
                  }
              ]
          }

      pool3:
        node_selectors: >
          { "node-pool": "pool3" }
        sriov_network_device_plugin_configmap_name: "sriov-dp-configmap3"
        sriov_network_device_plugin_configmap_data: >
          {
              "resourceList": [{
                      "resourceName": "dp_sriov_mlx5_left",
                      "selectors": {
                          "pciAddresses": ["0000:00:06.0", "0000:00:08.0", "0000:00:0a.0", "0000:00:0c.0"],
                          "vendors": ["15b3"],
                          "devices": ["1018"],
                          "drivers": ["mlx5_core"],
                          "isRdma": true
                      }
                  },
                  {
                      "resourceName": "dp_sriov_mlx5_right",
                      "selectors": {
                          "pciAddresses": ["0000:00:07.0", "0000:00:09.0", "0000:00:0b.0", "0000:00:0d.0"],
                          "vendors": ["15b3"],
                          "devices": ["1018"],
                          "drivers": ["mlx5_core"],
                          "isRdma": true
                      }
                  }
              ]
          }


  ################################### Worker variables ########################

  node_pools:
  - name: pool1
    image: eccd-2.22.0-node-image.vbi
    flavor: STD_72vcpu_327680MBmem_0GBdisk
    count: 18
    root_volume_size: 180
    # managed-on-host, ha-offline, unmanaged
    metadata:
      ha-policy: ha-offline

    # try max_pods parameter in this DMC 1.7
    max_pods: 150

    cpu_manager_policy: "none"
    reserved_cpus: ""
    isolate_interrupts: false

    use_trunks: true
    external_networks:

    # standard worker eth1
    - network: eccd1-ecfe-signaling
      subnet: eccd1-ecfe-signaling_subnet
      port_extra_properties:
        port_security_enabled: false

    # standard worker eth2
    - network: eccd1-ecfe-oam
      subnet: eccd1-ecfe-oam_subnet
      port_extra_properties:
        port_security_enabled: false

    # standard worker eth3
    - network: eccd1-ecfe-intersite
      subnet: eccd1-ecfe-intersite_subnet
      port_extra_properties:
        port_security_enabled: false

    # standard worker eth4
    - network: eccd1-macvlan-parent-1
      subnet: eccd1-macvlan-parent-1_subnet
      port_extra_properties:
        port_security_enabled: false


    labels: "type=standard"
    node_role: "worker"
    server_group_policies: ['anti-affinity']
    nova_availability_zone: nova
    cinder_availability_zone: nova
    pre_join_user_script: |
      #!/usr/bin/env bash
      set -ue

      mv /lib/modules/$(uname -r)/kernel/net/sctp/sctp.ko.xz /home/eccd/

      cat << EOF > apparmor-docker-pcc
      #include <tunables/global>

      profile docker-pcc flags=(attach_disconnected,mediate_deleted) {
        #include <abstractions/base>
        network,
        capability,
        file,
        mount,
        umount,
        ptrace peer=@{profile_name},
      }
      EOF

      mv apparmor-docker-pcc /etc/apparmor.d/
      systemctl restart apparmor

      function load_kernel_module() {
        for m in $@; do
          modprobe $m && echo $m >> /etc/modules-load.d/dm5gc_nfvi_preload.conf
        done
      }
      load_kernel_module ip6table_mangle fou

      # adjust kernel parameters for DM 5GC CNFs
      cat << EOF >> /etc/sysctl.conf
      ## update to kernel parameters required by Search Engine
      vm.max_map_count = 262144
      EOF
      sysctl -p

      # Label the worker nodes for PCC
      cfn="/var/lib/os-collect-config/cfn.json"
      node_index=$(jq -r '.deployments[0].inputs[] | select(.name=="node_index") | .value' ${cfn})
      if [[ "${node_index}" == "0" ]] || [[ "${node_index}" == "1" ]]; then
          label="pcc-mm-pod=controller,pcc-sm-pod=non-controller,"
      elif [[ "${node_index}" == "2" ]] || [[ "${node_index}" == "3" ]]; then
          label="pcc-sm-pod=controller,pcc-mm-pod=non-controller,"
      else
          label="pcc-mm-pod=non-controller,pcc-sm-pod=non-controller,"
      fi
      sed -i "s/node-labels=/node-labels=${label}/g" /usr/local/lib/systemd/system/kubelet.service
      # Set VirtIO vNIC queues
      cat <<EOF > /usr/local/bin/set_virtio_eth_queue.sh
      if [[ \$# -ne 1 ]]; then
          echo "Error: wrong argument. Example: \$0 4"
          exit 1
      fi
      new_queue=\$1
      virtio_eth_list=\$(ls -l /sys/class/net | grep virtio | awk '{print \$9}')
      for virtio_eth in \${virtio_eth_list}; do
          echo -n "Set \${virtio_eth} to \${new_queue} queues ... "
          max_queue=\$(ethtool -l \${virtio_eth} | grep Combined | awk '{print \$2}' | head -1)
          if [[ \${max_queue} -ge \${new_queue} ]]; then
              ethtool -L \${virtio_eth} combined \${new_queue} && echo "OK"
          else
            echo "SKIP: exceeding max \${max_queue} queues"
          fi
      done
      EOF
      chmod a+x /usr/local/bin/set_virtio_eth_queue.sh
      QUEUES=4
      cat <<EOF > /etc/systemd/system/set_virtio_eth_queue.service
      [Unit]
      Description=Set queue number for virtio NIC when multi-queue is enabled
      [Service]
      Type=oneshot
      ExecStart=/bin/bash -c "/usr/local/bin/set_virtio_eth_queue.sh ${QUEUES}"
      [Install]
      WantedBy=multi-user.target
      EOF
      systemctl enable set_virtio_eth_queue.service
      systemctl start set_virtio_eth_queue.service
    #post_join_user_script: ""

  # HT worker pool with Intel SR-IOV VFs
  - name: pool2
    image: eccd-2.22.0-node-image.vbi
    flavor: HT_46vcpu_87040MBmem_0GBdisk
    count: 0
    root_volume_size: 30
    # managed-on-host, ha-offline, unmanaged
    metadata:
      ha-policy: managed-on-host

    external_networks:

    # HT worker eth1
    - network: eccd1-ecfe-oam
      subnet: eccd1-ecfe-oam_subnet
      port_extra_properties:
        port_security_enabled: false

    # HT worker eth2
    - network: sriov-flat-left
      subnet: sriov-flat-left_subnet
      port_extra_properties:
        port_security_enabled: false
        binding:vnic_type: direct

    # HT worker eth3
    - network: sriov-flat-right
      subnet: sriov-flat-right_subnet
      port_extra_properties:
        port_security_enabled: false
        binding:vnic_type: direct

    # HT worker eth4
    - network: sriov-flat-left
      subnet: sriov-flat-left_subnet
      port_extra_properties:
        port_security_enabled: false
        binding:vnic_type: direct

    # HT worker eth5
    - network: sriov-flat-right
      subnet: sriov-flat-right_subnet
      port_extra_properties:
        port_security_enabled: false
        binding:vnic_type: direct

    # HT worker eth6
    - network: sriov-flat-left
      subnet: sriov-flat-left_subnet
      port_extra_properties:
        port_security_enabled: false
        binding:vnic_type: direct

    # HT worker eth7
    - network: sriov-flat-right
      subnet: sriov-flat-right_subnet
      port_extra_properties:
        port_security_enabled: false
        binding:vnic_type: direct

    # HT worker eth8
    - network: sriov-flat-left
      subnet: sriov-flat-left_subnet
      port_extra_properties:
        port_security_enabled: false
        binding:vnic_type: direct

    # HT worker eth9
    - network: sriov-flat-right
      subnet: sriov-flat-right_subnet
      port_extra_properties:
        port_security_enabled: false
        binding:vnic_type: direct

    labels: "type=high-throughput nic_vendor=intel"
    node_role: "worker"
    server_group_policies: ['soft-anti-affinity']
    nova_availability_zone: nova
    cinder_availability_zone: nova

    hugepage_type: 1GB
    hugepage_number_1gb: 4
    cpu_manager_policy: static
    # Only cpu '0' reserved prior to CCD2.18, however it needs more cpus to be reserved from CCD2.18
    reserved_cpus: "0-1"
    isolate_interrupts: true

    pre_join_user_script: |
      #!/usr/bin/env bash
      set -ue
      # Taint HT workers
      sed -i "s/KUBELET_EXTRA_ARGS=/&--register-with-taints=high-throughput=true:NoSchedule /" /usr/local/lib/systemd/system/kubelet.service

      # Bind SR-IOV VF to DPDK driver
      ifdown eth2
      ifdown eth3
      ifdown eth4
      ifdown eth5
      ifdown eth6
      ifdown eth7
      ifdown eth8
      ifdown eth9
      driverctl set-override 0000:00:06.0 igb_uio
      driverctl set-override 0000:00:07.0 igb_uio
      driverctl set-override 0000:00:08.0 igb_uio
      driverctl set-override 0000:00:09.0 igb_uio
      driverctl set-override 0000:00:0a.0 igb_uio
      driverctl set-override 0000:00:0b.0 igb_uio
      driverctl set-override 0000:00:0c.0 igb_uio
      driverctl set-override 0000:00:0d.0 igb_uio
      # Set VirtIO vNIC queues
      cat <<EOF > /usr/local/bin/set_virtio_eth_queue.sh
      if [[ \$# -ne 1 ]]; then
          echo "Error: wrong argument. Example: \$0 4"
          exit 1
      fi
      new_queue=\$1
      virtio_eth_list=\$(ls -l /sys/class/net | grep virtio | awk '{print \$9}')
      for virtio_eth in \${virtio_eth_list}; do
          echo -n "Set \${virtio_eth} to \${new_queue} queues ... "
          max_queue=\$(ethtool -l \${virtio_eth} | grep Combined | awk '{print \$2}' | head -1)
          if [[ \${max_queue} -ge \${new_queue} ]]; then
              ethtool -L \${virtio_eth} combined \${new_queue} && echo "OK"
          else
            echo "SKIP: exceeding max \${max_queue} queues"
          fi
      done
      EOF
      chmod a+x /usr/local/bin/set_virtio_eth_queue.sh
      QUEUES=4
      cat <<EOF > /etc/systemd/system/set_virtio_eth_queue.service
      [Unit]
      Description=Set queue number for virtio NIC when multi-queue is enabled
      [Service]
      Type=oneshot
      ExecStart=/bin/bash -c "/usr/local/bin/set_virtio_eth_queue.sh ${QUEUES}"
      [Install]
      WantedBy=multi-user.target
      EOF
      systemctl enable set_virtio_eth_queue.service
      systemctl start set_virtio_eth_queue.service
  # HT worker pool with Mellanox SR-IOV VFs
  - name: pool3
    image: eccd-2.22.0-node-image.vbi
    flavor: HT_46vcpu_81920MBmem_0GBdisk
    count: 6
    root_volume_size: 30
    # managed-on-host, ha-offline, unmanaged
    metadata:
      ha-policy: managed-on-host

    external_networks:

    # HT worker eth1
    - network: eccd1-ecfe-oam
      subnet: eccd1-ecfe-oam_subnet
      port_extra_properties:
        port_security_enabled: false

    # HT worker eth2
    - network: sriov-flat-left
      subnet: sriov-flat-left_subnet
      port_extra_properties:
        port_security_enabled: false
        binding:vnic_type: direct
        value_specs:
          binding:profile: {trusted: "True"}

    # HT worker eth3
    - network: sriov-flat-right
      subnet: sriov-flat-right_subnet
      port_extra_properties:
        port_security_enabled: false
        binding:vnic_type: direct
        value_specs:
          binding:profile: {trusted: "True"}

    # HT worker eth4
    - network: sriov-flat-left
      subnet: sriov-flat-left_subnet
      port_extra_properties:
        port_security_enabled: false
        binding:vnic_type: direct
        value_specs:
          binding:profile: {trusted: "True"}

    # HT worker eth5
    - network: sriov-flat-right
      subnet: sriov-flat-right_subnet
      port_extra_properties:
        port_security_enabled: false
        binding:vnic_type: direct
        value_specs:
          binding:profile: {trusted: "True"}

    # HT worker eth6
    - network: sriov-flat-left
      subnet: sriov-flat-left_subnet
      port_extra_properties:
        port_security_enabled: false
        binding:vnic_type: direct
        value_specs:
          binding:profile: {trusted: "True"}

    # HT worker eth7
    - network: sriov-flat-right
      subnet: sriov-flat-right_subnet
      port_extra_properties:
        port_security_enabled: false
        binding:vnic_type: direct
        value_specs:
          binding:profile: {trusted: "True"}

    # HT worker eth8
    - network: sriov-flat-left
      subnet: sriov-flat-left_subnet
      port_extra_properties:
        port_security_enabled: false
        binding:vnic_type: direct
        value_specs:
          binding:profile: {trusted: "True"}

    # HT worker eth9
    - network: sriov-flat-right
      subnet: sriov-flat-right_subnet
      port_extra_properties:
        port_security_enabled: false
        binding:vnic_type: direct
        value_specs:
          binding:profile: {trusted: "True"}

    labels: "type=high-throughput nic_vendor=mellanox"
    node_role: "worker"
    server_group_policies: ['soft-anti-affinity']
    nova_availability_zone: nova
    cinder_availability_zone: nova

    hugepage_type: 1GB
    hugepage_number_1gb: 4
    cpu_manager_policy: static
    # Only cpu '0' reserved prior to CCD2.18, however it needs more cpus to be reserved from CCD2.18
    # Set reserved_cpus to 0-1 as a part of isolated configuration in CCD 2.21
    reserved_cpus: "0-1"
    isolate_interrupts: true

    pre_join_user_script: |
      #!/usr/bin/env bash
      set -ue
      # Taint HT workers
      sed -i "s/KUBELET_EXTRA_ARGS=/&--register-with-taints=high-throughput=true:NoSchedule /" /usr/local/lib/systemd/system/kubelet.service
      # Set VirtIO vNIC queues
      cat <<EOF > /usr/local/bin/set_virtio_eth_queue.sh
      if [[ \$# -ne 1 ]]; then
          echo "Error: wrong argument. Example: \$0 4"
          exit 1
      fi
      new_queue=\$1
      virtio_eth_list=\$(ls -l /sys/class/net | grep virtio | awk '{print \$9}')
      for virtio_eth in \${virtio_eth_list}; do
          echo -n "Set \${virtio_eth} to \${new_queue} queues ... "
          max_queue=\$(ethtool -l \${virtio_eth} | grep Combined | awk '{print \$2}' | head -1)
          if [[ \${max_queue} -ge \${new_queue} ]]; then
              ethtool -L \${virtio_eth} combined \${new_queue} && echo "OK"
          else
            echo "SKIP: exceeding max \${max_queue} queues"
          fi
      done
      EOF
      chmod a+x /usr/local/bin/set_virtio_eth_queue.sh
      QUEUES=4
      cat <<EOF > /etc/systemd/system/set_virtio_eth_queue.service
      [Unit]
      Description=Set queue number for virtio NIC when multi-queue is enabled
      [Service]
      Type=oneshot
      ExecStart=/bin/bash -c "/usr/local/bin/set_virtio_eth_queue.sh ${QUEUES}"
      [Install]
      WantedBy=multi-user.target
      EOF
      systemctl enable set_virtio_eth_queue.service
      systemctl start set_virtio_eth_queue.service
  subport_data:
    pool1:
      subportsets:
        subportset1:
        - network_id: 050f7142-418c-4e58-bade-25a091f2528c
          vlan_id: 352
        - network_id: c4bb1f2b-7997-448b-89ae-ffb2c20f87e2
          vlan_id: 601
        - network_id: cff18593-a9e7-44b5-a0fc-2d51eabcd47c
          vlan_id: 602
        - network_id: 02d81e7e-92f6-46e9-acdf-ce6d51bc52bc
          vlan_id: 452
        - network_id: f611eb68-f831-456f-a02f-f141d97b96ca
          vlan_id: 453
        - network_id: d54a71cd-9dcf-404b-a868-3cef9ef80143
          vlan_id: 401
        - network_id: c40c4b42-553a-4747-ae2d-f4eabcd2ad89
          vlan_id: 454
        - network_id: b9250ed2-5dc4-4aee-abfd-dd02ced7d2f7
          vlan_id: 402
        - network_id: 2bf90ec5-2f08-4d24-9f67-472b65d91bd0
          vlan_id: 652
        - network_id: 6baebc28-e061-4a2f-97c3-26fc4f3807c8
          vlan_id: 603
        - network_id: ec25a749-3dff-4dcc-b6ca-6b92de7f4925
          vlan_id: 455
        - network_id: d6024fc4-7dee-4a88-8c39-17aa33d7b889
          vlan_id: 456
        - network_id: 09541301-0510-459f-b7dc-93f4ed114a0f
          vlan_id: 551
      nicsets:
        3:
        - subportset1

  ################################### Logger variables ########################
  logger_enabled: False

  ################################### Other variables #########################
  nova_availability_zone: nova
  master_server_group_policies: ['anti-affinity']
  director_server_group_policies: ['anti-affinity']
  cinder_availability_zone: nova

  os_cacert: |
    -----BEGIN CERTIFICATE-----
    MIIFgjCCA2qgAwIBAgIQXfAZDd+9XJpFa1GJ2eqreDANBgkqhkiG9w0BAQsFADAV
    MRMwEQYDVQQDEwpFR0FEUm9vdENBMB4XDTE4MDIwOTE1MzMyM1oXDTM4MDIwOTE1
    NDAwMlowFTETMBEGA1UEAxMKRUdBRFJvb3RDQTCCAiIwDQYJKoZIhvcNAQEBBQAD
    ggIPADCCAgoCggIBAM5hqGswhA9sZstmTwCCp18z7Y+OSUGzFSrJ/ocI4h9UsusA
    40yOvmt4XnNxb6oGFsEOE+8eHzaCE00TlwS9nLGq6x+cj8nDuLYI66KEimR1xdkt
    3mwJbSZ/4tuulgvyUcARB+xcRSsyLWraboxFa41VY6yCeBfs4FKyrkmGmhFxzknN
    jKQQRkfRQz3wyggWebSa1Aa2DZdMa0BSn+ZnFcMyhOamLxj5RVvTLwuoqhSCfVU5
    QW5ioLQ85DFg4667VClP+gDQ0F2XoLBmUPDmSXrlHZ3x+yA+ls0rdh79N84dq9hU
    2Y3zApqyohjeFCj9F0DiGMRwQ8cj9PhMHp9mCreuS6ZMXk5Phqil5gyyf1Blqg9Y
    21JWUoag+CKFlTbL6yRRauhbLCFTxZxpc6noG0B5fXTEftIl040jQ0XFcx6BOBru
    DESkdhCuT4LZw4tOU79v5OVxLAcZOWsC+l/gt2gRRToz3HFFR4R7vDzVJKom5d22
    dwy7v17kVcIsbMNndJMELz8UXK7tbHioBNnW8Sp7sRFrxz/lPzUF6eJFucgrpnng
    n8xBoiBbZOWu7EFttTOE2pKpPPGFbl+c0EbyUa71RxidRsBKjy3aTDwT967q1h/v
    uZDecSQH3JDPqH66MvP31YvLOLxSczWYYLC0+R23SslEgRFOxmtF4YODM4KPAgMB
    AAGjgc0wgcowCwYDVR0PBAQDAgGGMA8GA1UdEwEB/wQFMAMBAf8wHQYDVR0OBBYE
    FFZFQloUqLksRP6bNcTb7i85gk3QMBAGCSsGAQQBgjcVAQQDAgEAMHkGA1UdIARy
    MHAwbgYEVR0gADBmMGQGCCsGAQUFBwICMFgeVgBUAGgAaQBzACAAUABLAEkAIABp
    AHMAIABpAG4AdABlAG4AZABlAGQAIABmAG8AcgAgAGkAbgB0AGUAcgBuAGEAbAAg
    AHUAcwBlACAAbwBuAGwAeQAuMA0GCSqGSIb3DQEBCwUAA4ICAQBiBuc9JLpv0+nx
    eY5Np7X0HPODG4AdPepVpwh/JUh07G8eClq6ZFoejjiIGpLvMN+Lhfg8Q+hoh7Yb
    z2Gf96TpyAsjx4lmqIgDNfBdf7G0wDf+irP6pifCPBzu2T5519WztvaiZfQoPEz+
    CXm3/yiSKBQUx705fPctx91TjkQEbiBFU3hN9bmw5VkM+2HN7eDJUIUVNXXaTec0
    6M1RhLB2vLVinACpamylLIzShLZpDyk1VI2tniyia+sJulDdpu6kY3LuMr7s97Gj
    UqFHn3PO8lXwYqueJ5xeOjMIKzaliw5n58mDYzpt1uu0u9i0w/8nJpGl4iYwcgBn
    ZPmKet3KG+9LIh/JUdR2KMFv706E4Z24B/QCjbRFqjzS4BDXKmcZV84/HwA48hwa
    yUTWsWZXIibEmvyQcZo9+943o3jkiFYDZZfmQ2a2bzvcHsrNa4ptJYWaBEm2NQWa
    F6pPdUr9uerqyOjRHPkPyd711P7gYC6MWlEKi9JME6lbvanFOwwwh+4nhNnk1iw8
    335Q2U+S8WCzPo7Mzn2DGfa6bdO6sOChXgM//QbaL9GH0LYO8O6XWfS0HSztnBFB
    etCUFffhDdzPkIxfmd7ylGgZSbls2+KFjIgF2M9gKkglmzf5qjNhA1jjHiaUyRTZ
    tRq4ImVO9Ddg1VngqJRZvro+zlH3pQ==
    -----END CERTIFICATE-----
    -----BEGIN CERTIFICATE-----
    MIIGhTCCBG2gAwIBAgITJAAAAAgZo51IZLn3FgAAAAAACDANBgkqhkiG9w0BAQsF
    ADAVMRMwEQYDVQQDEwpFR0FEUm9vdENBMB4XDTE5MDUwMjEwNTgzNVoXDTI5MDUw
    MjExMDgzNVowRzESMBAGCgmSJomT8ixkARkWAnNlMRgwFgYKCZImiZPyLGQBGRYI
    ZXJpY3Nzb24xFzAVBgNVBAMTDkVHQURJc3N1aW5nQ0EzMIICIjANBgkqhkiG9w0B
    AQEFAAOCAg8AMIICCgKCAgEAorS/FxfmNVug9Q963xTydgf5JD/XHEQ0HAypY0Mw
    omgZA1Sj1cRFjFzqVbZlQB/d0RUeN8K9+wzpXl2sDiDTYFKc4v3sesmVrT+VHOtf
    2G1Wuw+7OjMURFhoGNz56vDEL10KH86ViPF0Rx3R5/myKYgn/AX6rdVPSsBPyhAe
    YDdsNX3yWrvxr6x3P5Ze8L5cfgDmw1NR3sR3j9mJSKxEpJHo/6z+wh1xOgV7z8Kj
    Tzdl0+103ZvPw8wpnX3JGwOy5O/lIXMFctMEvZoD/eIerPtYEdjk8HrGbWWu66VK
    EoXopl0/3MbPypVdizPcRBq7ldaCDDdLMi2LjFvZbD0UwHHjR/piIE/YsdPLxi1o
    KSO2vPJHSPjpQ0JtP0gcAGR1wSFQCiOzklupMzZ1PMSySbT04woRnUDdTu9BMzIi
    OV5scI+KtJ1JmPqHq3nCS2nkA0grSNbsbB0bz/mrJLX+wH3CXNj0r0gBIDKlaPQb
    8UzrahG3uuIGk4/x8gnU/wh5WYeU89o+3neIgXySK5chnjfevWBwYVF1vddYltpf
    DFZYuFwT+zefeMEiPnseW6JSnRvNljz1XsBlPC++UydVJDGkp+WzeCR46J6iCVeK
    kWKoLv+BtNIbue7SJw0GdGWBJ0DzKdPBmp+Ya5YPDE1g8SqrfdlO61ykJQ1L6EGf
    Sb0CAwEAAaOCAZowggGWMBAGCSsGAQQBgjcVAQQDAgEAMB0GA1UdDgQWBBSBekIQ
    iZ64URB+VptBwt+veMU9JDB5BgNVHSAEcjBwMG4GBFUdIAAwZjBkBggrBgEFBQcC
    AjBYHlYAVABoAGkAcwAgAFAASwBJACAAaQBzACAAaQBuAHQAZQBuAGQAZQBkACAA
    ZgBvAHIAIABpAG4AdABlAHIAbgBhAGwAIAB1AHMAZQAgAG8AbgBsAHkALjAZBgkr
    BgEEAYI3FAIEDB4KAFMAdQBiAEMAQTALBgNVHQ8EBAMCAYYwEgYDVR0TAQH/BAgw
    BgEB/wIBADAfBgNVHSMEGDAWgBRWRUJaFKi5LET+mzXE2+4vOYJN0DA/BgNVHR8E
    ODA2MDSgMqAwhi5odHRwOi8vcGtpLmVyaWNzc29uLnNlL0NlcnREYXRhL0VHQURS
    b290Q0EuY3JsMEoGCCsGAQUFBwEBBD4wPDA6BggrBgEFBQcwAoYuaHR0cDovL3Br
    aS5lcmljc3Nvbi5zZS9DZXJ0RGF0YS9FR0FEUm9vdENBLmNydDANBgkqhkiG9w0B
    AQsFAAOCAgEAk6hklv9NPua9UImj08ILq43l9SgFhg5+y1pRsrUwdTmFANkL0dxi
    TOF93QwFTfwOs1xJr2nzvCsoJ2Rvc04VQlONwiTqn1kMs9vJr2fuQg9hf2O9l0ss
    AftlHq+UxbeIhloGTyJgleJUc2/+pm4UJn8fhZvNZywL8ok4xr5HQRH5xQz21X4i
    VF9RJlvZbsdWLX1iJMs8+oZJQmhRxoHsPylg+24xLuKFuva7BevDub4HpF6utXeE
    qRMN8Yd1HPg55abf8wEqt1VqNp2erZfLETdf59aXdznqeI+ZJl8gQZwBsOlSANct
    n0nj306BlYm6jthXb7T/Bgau48qyrH/vEnwawZ+RH2q1UvGiAMY1/L5PAP4LGubP
    q5VHsibvoQz+jld2bWM4X8y7Z93s33/elZo26pNuYWiE67IVmZntoHJZmq0RfI7V
    cG27eExeNXYc7T0HheK32CguyDsDUURHOMfMzUJBzDnZ8zQWWeOsKCut9xcpxpPW
    7XQB+iH59lFgRF0WW3H4Z3jh7Kvfbr6XsbzEx/onzLaEVrXBf1xKlXgV5rZW69w1
    It5HdRmqOsfW32Bdz9JehI+Z/Iql9UFDsM1lS9dWfMj4Kv3ALpmG5uS3SjU5y1+e
    7EaQhOp85z15UYt9yoS7KqaNZ07bZb2uzBs32lN/Bn1vwPoLA60kI48=
    -----END CERTIFICATE-----
    -----BEGIN CERTIFICATE-----
    MIIDazCCAlOgAwIBAgIJAIBmOg2fwGLSMA0GCSqGSIb3DQEBCwUAMEwxCzAJBgNV
    BAYTAlNFMQswCQYDVQQIDAJTVDELMAkGA1UEBwwCS0kxETAPBgNVBAoMCEFBLCBJ
    bmMuMRAwDgYDVQQDDAdSb290IENBMB4XDTIxMTAyODA5NTYyNloXDTIyMTAyODA5
    NTYyNlowTDELMAkGA1UEBhMCU0UxCzAJBgNVBAgMAlNUMQswCQYDVQQHDAJLSTER
    MA8GA1UECgwIQUEsIEluYy4xEDAOBgNVBAMMB1Jvb3QgQ0EwggEiMA0GCSqGSIb3
    DQEBAQUAA4IBDwAwggEKAoIBAQC97lDYNH7fupczpDmo7uY3v6Tvggdb7L5oUCAM
    PTIA2Qnm42lPMXJcVU6pDC60dmsGor/9deZ4J3wmGHNrcz1ok9NfNLBecquCOhVH
    uqTNnM7fZT2VQKlPbU6lAH8dnLYPHIDt5mUivVgCsQz1q3rwWMCB4Oic224y9N8k
    xSL4otgzAEgHNL3y87cugqhjGg4T3r/VJYIQLTe8g4cLS4zvsBreFqwy7y8C4cWI
    mhzpkvGoCbQts4eJLW2x0Nm1CeEySjxGVLxS2rUvuUPS8CrMgaBlR3GmTXnS4Jk4
    s6ctsqdjupktSpc3fPDdDJ3qz+3Pw/vHeIJ76mvXShCYf/rnAgMBAAGjUDBOMB0G
    A1UdDgQWBBRKq1p6MWOaDHx/6AXR/D7GeB/VBjAfBgNVHSMEGDAWgBRKq1p6MWOa
    DHx/6AXR/D7GeB/VBjAMBgNVHRMEBTADAQH/MA0GCSqGSIb3DQEBCwUAA4IBAQBi
    tbrg2GWiuu1p8dmwjJYyTVqlCoelY926176uW6Hg/ba28NSFQvWYXil1iqhb3zCe
    w6cKcDnth1ErzpcfZrwIZe8x3h9o7tQ37Xx+gkkBFcu5lvJAg+6PLRNurbVkO+YZ
    MRjbCZ2nDLbKLqEFUcXachoQcfGzbBBWOG/lvPIc09kR+0njc8d6P2KkoWfKak9o
    8PScXGRcf9uiWwpaLPod7JBtd9afhOJj2iI35eVKnG1qCyhiS9YSBdqhM6uifyA7
    fhbtfR3sRilfuEku28Kx0ZyNv7sIQO50qGFoZ8Do2oE8sgoKUOuKFGGy4JDJI1Qz
    ejUDiJllQlEqdGUGfk+F
    -----END CERTIFICATE-----
    # evnfm self-singed CA certificate
    -----BEGIN CERTIFICATE-----
    MIIGCTCCA/GgAwIBAgIBADANBgkqhkiG9w0BAQsFADCBnjELMAkGA1UEBhMCU0Ux
    EjAQBgNVBAgMCVN0b2NraG9sbTESMBAGA1UEBwwJU3RvY2tob2xtMREwDwYDVQQK
    DAhFcmljc3NvbjEVMBMGA1UECwwMUEMgU29sdXRpb25zMRgwFgYDVQQDDA9UZWFt
    Qmx1ZXNSb290Q0ExIzAhBgkqhkiG9w0BCQEWFGtlbi5zaGlAZXJpY3Nzb24uY29t
    MB4XDTIwMTEwMjA5MzMxNloXDTQwMTAyODA5MzMxNlowgZ4xCzAJBgNVBAYTAlNF
    MRIwEAYDVQQIDAlTdG9ja2hvbG0xEjAQBgNVBAcMCVN0b2NraG9sbTERMA8GA1UE
    CgwIRXJpY3Nzb24xFTATBgNVBAsMDFBDIFNvbHV0aW9uczEYMBYGA1UEAwwPVGVh
    bUJsdWVzUm9vdENBMSMwIQYJKoZIhvcNAQkBFhRrZW4uc2hpQGVyaWNzc29uLmNv
    bTCCAiIwDQYJKoZIhvcNAQEBBQADggIPADCCAgoCggIBALpFSt2RPftYwPHaaX0O
    li/g52VnHmyACmG1egnPA4efRptfFqsm9lHH2byigSech+zfLXCarVErdu1/yvRG
    zYaLgH/76XtOVXpGYa3aXBksBbkTnIRmgtq11iIdOKXUrVbVt4HZ20EJxrFrMs9n
    mxy7dvhWM1KrSVSUqN840DSWJxeFWWgEZ7vTgRhnuAD2FUqsZ6CRvt2iL31ZNIco
    dqOUQF9chZLWgprEDgRmJD4oLjVoWe0kS/zRR2G3GB9J28xs9ZjSp38CtTgqBzyv
    gWua4Im5KyFIJUeH4a9NRYZWry6I9sxAzIYVt7ki3TAP+K2YsGg9dD1GwKDR+5Zy
    7EVuri9hqR5YFrvvdNAjWrJvymQXaDdmXUGa7Og8p9IzM+lNL86dDf3BsyeqHJrq
    m31OMHM3MCVfc0zUu96LnLj2wqTdKa7MaWyfA7KO38wIACBvO/SNCCzCXpADJRED
    A3n219y6QxmM+DH35XVBTvJkM5UxPHoa8rhZsuLhobIyNU/Kuk6JAm/lueYzOG9k
    lLmaaSasdlNJ/Pqf4MBOB1UKPVsJsravFaQ7BmOLp3R0XyhoNrnmd/OOxU6r1TH1
    wh5vx2CAOO4tujvnihgeADU8yQdwDKBkOtjjV0TdY0MFxecLowtmEnrKfIO9kSS3
    tpnAgdBgfq7eOYvsrKw31AqxAgMBAAGjUDBOMB0GA1UdDgQWBBTqkIEarX28jATB
    MBsv/Vse5jCebDAfBgNVHSMEGDAWgBTqkIEarX28jATBMBsv/Vse5jCebDAMBgNV
    HRMEBTADAQH/MA0GCSqGSIb3DQEBCwUAA4ICAQB73tG3G1mZt93j5MoRXimsB6sO
    q47A/lZiyJKqJiLIeSq1V8GOPwqfQyNAe40inhLWHYky5rPfhhXw+FlyW4zotv7W
    ZygTjtCwa7EbawUMOoG9YcMhK9NZx/EoPT0cs4BK6UvajpH28Kl5+fyvSLN5ZY7K
    arBIMUWNVs7m3O9e5doLx6e98soj2U5PkGq1AB2xgwX7F0spIj3Yg3WS4C5z5KkF
    b1cviVgHuFnxJNGu5nY+h08+i5cZNu/Uc1M9pkYthlfnhpyPlONxD9rTpDm5LEkU
    3af9zyqwJuuunWDJMfKKZGPB5BCgfM6hBw4D3iLWTWxrHfjNghNVqCh6K+L+GdfD
    hzT/LVSHW/bOjHh+wAVGFcFxxXedXxq/prj1ZwflPI7XvxuCwj1EVA630FIBSC6/
    xWq8mHoECXXp839Gccd3tmeauELrchkty6OaBpwHg/dB44GEByJz/T+lbLIOZLXd
    K+kpgaN4TmA5wtlQPMq0HQLvEax+cCUYtE78QSe+86mV/2Q1EeJNEesT7p5C2U4c
    aQZbPQ6YgvRZtvPD/3CrQS9CD0DJwaCrj2dEISb4lEj/fz5wlLETyS7X/0KYUMKR
    UmqyKwz2Zs9jM7By0HxM1i+0f/ovMDuKuQ6IQTROEu0TZZVH6KRsDuujv+/VJ2Nb
    szlrRw3la1H71eU9UQ==
    -----END CERTIFICATE-----
  ca_cert: |
    -----BEGIN CERTIFICATE-----
    MIIDAzCCAeugAwIBAgIJAJIpqtmMSIhHMA0GCSqGSIb3DQEBCwUAMBgxFjAUBgNV
    BAMMDUt1YmVybmV0ZXMtY2EwHhcNMjExMDI5MTIyMTMxWhcNNDkwMzE2MTIyMTMx
    WjAYMRYwFAYDVQQDDA1LdWJlcm5ldGVzLWNhMIIBIjANBgkqhkiG9w0BAQEFAAOC
    AQ8AMIIBCgKCAQEAk6m1Sxy+jsKf/jbKCSINUGEiAhKvD1phHcgsF4gs2x39pxaP
    4KEHaZr8bdESaRU/pOG+/ux+uwCnLqiMww+vfMeduypehH+q8TZ/qO6XJgs0uW++
    NSRgHUZvepztNbidcB6d9fULIotIKhXThUY2lmyuAe5OVexStEvdOtXQLBrqz+Z3
    iECY8AmwJIC1tIx03iXFMFgOVs3rXK6vlOBG1plwoaMNmYkbssxZtQO4ghwa4lqC
    lUKPW86pUT04dNUSUnV5eKoBCqw6Y2qYQx6qbp1A5YbYnlZIObGa8TjiqiJJ0B6n
    4hCpMpjN4R61a9C4mLXn1O0qiXS5PQJnt3MuwwIDAQABo1AwTjAdBgNVHQ4EFgQU
    oiBs1khHGt0Bmp12HDQioA0Dyb4wHwYDVR0jBBgwFoAUoiBs1khHGt0Bmp12HDQi
    oA0Dyb4wDAYDVR0TBAUwAwEB/zANBgkqhkiG9w0BAQsFAAOCAQEAZ3Js+82aPVXk
    Kytbddmi/k4DF8tekfdnrG+6yM0mstpzh+xPwlIBZaIhXgqOD3oMGPataKmyc+9B
    cfgtU4Jv1pl8kC0rnQwb97EBSvCKCLPCNsCaoyLKzFXeaGL+lpop8TRQHGcSYX2f
    se6ftXuS96jMtpsyzEb6xQqcbwgp30VTQZfKbXPu0j9J3q0oT5VOOTjOJ9KOl9AS
    iqcJ6rKlW00IBllLYyHJrdSdeNE3sei8Q4cYQT6r47Eje0vp6bgfsETk1Icj/PDm
    su/sG+15vdphIOxSyiJkZWwBZCTf8tzzvB3TYdUwm4sTw8i11xWGxijAhljypYWx
    MT1a7mbbsg==
    -----END CERTIFICATE-----
  ca_key: |
    -----BEGIN RSA PRIVATE KEY-----
    MIIEpAIBAAKCAQEAk6m1Sxy+jsKf/jbKCSINUGEiAhKvD1phHcgsF4gs2x39pxaP
    4KEHaZr8bdESaRU/pOG+/ux+uwCnLqiMww+vfMeduypehH+q8TZ/qO6XJgs0uW++
    NSRgHUZvepztNbidcB6d9fULIotIKhXThUY2lmyuAe5OVexStEvdOtXQLBrqz+Z3
    iECY8AmwJIC1tIx03iXFMFgOVs3rXK6vlOBG1plwoaMNmYkbssxZtQO4ghwa4lqC
    lUKPW86pUT04dNUSUnV5eKoBCqw6Y2qYQx6qbp1A5YbYnlZIObGa8TjiqiJJ0B6n
    4hCpMpjN4R61a9C4mLXn1O0qiXS5PQJnt3MuwwIDAQABAoIBAB/Hwb4VxKmDF94l
    upv2mTj4ftJFZSn/wLEhOOQjqcvLC6GfiH3HCopfEf8spTK6oQUClRHWonvq6xBu
    tETzJkjLyZXTy53mwErD+PvJxfwpI0LZ2u0jFBCY7TtNdPw/7JqJ7GNYRvo7Ud/E
    7M5dSzha0aSWvoNdpnspbMbR58jl+nwpJAcGBtjqpizK/GC1w5YY55pcCXVvVBj4
    e/9Y2tU0go9rBcAP4Nphd4WqSF3IuLDztSe859qQuzShM1JB8u42AiCrt9O2Xr+a
    KXhmT5RfV+NAI1Xd1/7i5mum6No7cuvDqApSyF9NUNARhGdp8m0ks0hTdJvbTt/7
    fSLBP9ECgYEAw9UwUG+oYHBeT8M25n5KxUAt3/GM5ShPkmHW1BcHKZphTrxSzWWV
    6rQvPwTSLhZukI7NMkj+bYPo0dYZeZ8othOfo4Q2bnBMjlr8ojjQUG3nghM5U9bI
    tXqRTKUzB4MJ+eSETOhpEsF8dRQqzhsa0DG936A5893tBhY8nJS/sokCgYEAwQfS
    hGHKerD5DhmTjFFsiCTrd+ztZPCNh3SdzCPdd4KdhTGH/DHjPUuZNC+gFtxnRk7A
    1qITiPX2CQp+8rKoZnqaXx3/qQvXZYENqHVtziEuU5ngEOTvF+7l1ycsV5oV10/6
    VX6RO56nWj7fKw0BmDKRlojUAtRCX4yV0m1hM+sCgYEAlZFP0z9UdEOeZIEOf0BZ
    zwlTgES2fZcqHv0G3PmxiL7WLxJ6k1FRDO8NXq5J8NVFILdQVWWWnQIwYYOJ9W/v
    4hBezr597B9s+RTY+vU1dRhbxubz0+qpZKH2M3nA3a3hgrBbs3mWNrL8YVdD0iK1
    eeljwVZYrL8E9q8pH9YkCzECgYEAnWB7SmNQaeePpudDXMul6Pnw4L3SbtGREjcJ
    0D4NejqLouRXMk6aLodMSTfrFbhsmu3qWxOZSbj+9zAPgGAY3DRpV9NldbWpz6AA
    OrBC7iQvWQnDnQKJzMGBsoDFrkZ6OCmEt9+RGUKovvMzkC3ydtwDxu+XqxbTZoOA
    QyfacDECgYBhyLlB9gtq7L3zPWS9vLV/dn6LhHYAYx2/Yab79h+GIM2cMZq3kic3
    UP/8/Wh4RfqJkXicM2S93TGhSiwV3D6w/waENEYBrj0PQFn5EK5/97hJYNbOP56Z
    JLTEJe+PKdrLx90mLWS4absd+Ysfhs9inYJLZzZ+90z0WnqnGHc8Ug==
    -----END RSA PRIVATE KEY-----
  public_key: >
    ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQC/iAjVdUiZ1iyIdJFUp+BEo0pq9GRtbkvWjmHWdvEVZmaOqof6Z8eeIighl2M3nzuxU1n0BB7HO/Fua1UQ74fhniAuTWRACSrS5bY+/LzgRS7pCeM9KXdtDDoJaxlr4Yqkefi4S59Ht/MfY+fpq6yNANdQK8QL7lsWJR6otMgQytoIl4AyvJu3KW/w4QJsJnNvmS3AhONS5UtJt9fRdPQ9UTTrNTplipX0RoL4P6Mk26tmHxbSMwVISGzanpZU4hCtWrxRwKxPKkDrfuhgTn0kVWtgDdhhhL8SsSkbzg7dhDZAQELp9HACi8LH9bynO21/QRFPfHp9f3wA2saxKxxp ceeadm@cic-2.domain.tld
  ################################### Optional variables #########################
  # Custom root password for console access
  # must be sha-256 or sha-512 password hash type
  # update the below value to set the root password
  console_password_hash: '$6$riMvNtbPehi.s.rs$vM07tC/sCQ365l9FAVYXMNHZZe6sFdy.aNMCtTO4u36pmehasvntocVfGKpeV.5aDzgle0lqSsGDBRd6YXZdd/'
